% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

\documentclass{acm_proc_article-sp}

\begin{document}

\title{Building a Classifier to Discover Sentences Containing Adverse Drug Reactions}

%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{1} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Vignesh Raja\\
       \affaddr{Department of Computer Science, College of Engineering}\\
       \affaddr{University of Illinois at Urbana Champaign, Urbana, IL, 61801}\\
       \email{vraja2@illinois.edu}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
When the Food and Drug Administration (FDA) approves new drugs, it often takes years of public use for exhaustive adverse drug reaction (ADR) lists to be constructed. Oftentimes, the ADRs reported to the FDA are of poor quality. In recent years, patients have been posting their experiences with drugs on health forums, blogs, etc. To take advantage of online information, we need to be able to discriminate between text that contains information about ADRs and text that does not. In this paper, we create a classifier trained on sentence-level data from Ask a Patient, a website allowing patients to post about their experiences with specific drugs. We use unigrams, bigrams, trigrams, dependency parse information, and side-effects crawled from Side Effect Resource (SIDER) as features for our classifier. We perform evaluation using 12-fold cross validation on the Ask a Patient data which yields convincing results.
\end{abstract}

% A category with the (minimum) three required fields
\category{H.3.3}{Information Search and Retrieval}{Information Filtering}
\category{I.5.2}{Design Methodology}{Classifier Design and Evaluation}
\category{J.3}{Life and Medical Sciences}{Health}

\terms{Design, Experimentation, Human Factors}

\keywords{Adverse Drug Reactions, Machine Learning, Support Vector Machines, Feature Selection, Side Effect Resource, Ask a Patient} 

\section{Introduction}
%Define adverse drug events
%Talk about ISMP report discussing FDA problems
%Talk about relevant work
An adverse drug reaction (ADR) is defined as an unexpected or dangerous response to taking a medication. The distinction between an ADR and a side-effect is that the latter includes positive outcomes. The Food and Drug Administration (FDA) has an ADR reporting program called MedWatch which serves as the government's primary pharmacovigilance tool. Although MedWatch has an easy-to-use interface for submitting ADR descriptions, the data suffers from ambiguity and incompleteness. 

When a patient who is using a medication is contacted by the drug's manufacturer and the manufacturer learns that the patient died, the company is forced to report the death to the FDA in all cases, even if the drug has no suspected role in the death.  According to the Institute for Safe Medication Practices (ISMP) 2014 Quarterly Watch, it is estimated that in 28.5\% of reported deaths by pharmaceutical companies, there is no information clearly defining whether the drug at hand contributed to the deaths. Similarly, in several other cases, there is ambiguity about whether a drug worsens a patient's health condition, or if it is just a result of the ailment the patient is suffering from. 

Many times, drug manufacturers will also report ADRs directly to the FDA. In theory, this is an effective way of obtaining ADR data. However, in practice, these reports are of poor quality. Specifically, the ADRs reported are often common health problems such as ``cold'' or ``painful injection'' that aren't unique enough to specifically be attributed to the drug.

Additionally, for drugs newly approved by the FDA, the initial list of ADRs is not exhaustive. Clinical trials are limited to small groups of people and are conducted over short periods of time and as a result, every ADR may not be captured. For new drugs, it is important to find ADRs as soon as possible to limit the number of people afflicted by dangerous side-effects.

Using the web as a reference for medical problems has become very popular in recent years. Forums such as HealthBoards allow users to ask questions about and provide testimonials for many different drugs. Websites including Ask a Patient allow users to share the side-effects they experienced with different medications. Additionally, simply querying Google with the search terms ``<drug name> side effects'' or ``<drug name> adverse effects'' returns many different pages containing potential ADRs for a specific drug. 

Consider the scenario where a new drug is approved by the FDA with only a subset of the full true ADR list published. Suppose now that a user of this new drug experiences nausea, which has not been included as an ADR for the drug by the FDA. In addition to visiting the doctor, the user wants to consult the web for medical advice. The user posts on HealthBoards listing nausea as a condition he is experiencing. If more forum members reply to this user's post stating that they also are experiencing nausea, we recognize that nausea has a good chance of being an ADR of the new drug. Similar scenarios may also occur on other websites. We observe then that in addition to considering ADRs submitted directly to the FDA, we must also make use of the vast amount of drug data on other health websites to quickly and accurately construct a complete ADR list for a new drug.

An eventual system will continuously crawl Google to extract ADRs for newly FDA approved drugs. There are two main parts of this system. The first part is a classifier that discriminates between sentences that contain ADRs (positive) and sentences that do not (negative). When crawling Google results for a newly approved drug, our classifier will allow us to find only sentences that contain ADRs. The second half of our system will use the positively classified sentences to extract ADRs that are not on the FDA's list yet. In this paper we describe design of the first half of our system, which is the classifier. 

The rest of this paper is organized as follows. The next section surveys existing work in the ADR detection domain. Section 3 describes the features used in classification. Section 4 discusses classification results and error analysis. Finally, Section 4 concludes the paper and discusses future work and improvements.

\section{Related Work}
A significant amount of work has been done on ADR detection systems. Carino and Lambert et al. \cite{Carino} share the same goal of discovering unrecognized side-effects. First, they construct a neural network to classify web pages as either being relevant to a query drug or not. Some features that they use for classification include the presence of diseases/symptoms, the distance between keywords such as ``side-effect'' and a disease/symptom, and the absence of expressions such as ``no side-effects''. After retrieving relevant webpages for a drug using the classifier, a fixed-length sliding window of contiguous complete sentences is used to determine the most relevant passage within the webpage; more specifically, this is the window with the largest number of relevant features. Our approach differs from that of Carino and Lambert because we iterate through every sentence in the page and classify each sentence as positive or negative and finally retrieve all the positively classified sentences. This allows us to retrieve a set of non-contiguous sentences. On the other hand, because Carino and Lambert use a sliding window, the set of sentences they extract is always contiguous. 

Gurulingappa et al. \cite{Gurulingappa} worked on the task of classifying sentences as containing adverse drug events (ADE) or not. A distinction between our work and that of Gurulingappa's is that ADEs are a superset of ADRs; namely, ADRs imply a causative relationship between a drug and a reaction while ADEs refer to any injury occurring at the time a drug is used. Gurulingappa uses similar features to our system such as dependency parse information and n-grams, but to our knowledge, does not use mutual information to perform feature selection. In Gurulingappa's work, classification is done on sentences in medical reports, while the sentences we use are from Ask a Patient, which contains user submitted drug reviews. Typically, medical reports use more technical terminology than what a person would use in a forum or more casual website.

Sampathkumar et al. \cite{Sampathkumar} used HMMs to mine ADRs from healthcare forums. Their system has three different modules: information retrieval, text processing, and information extraction. We pay special attention to the information extraction module because it is where relationships are established between entities of interest such as side-effects and drugs. First, to perform named entity recognition (NER), a dictionary is created out of side-effects and drugs crawled from Side Effect Resource (SIDER). With the output of the NER system, HMMs are used to do relationship extraction, where relationships exist between named entities. Sampathkumar treats the problem as a sequence labeling task where the labels for tokens are ``drug'', ``keyword'', ``other'', and ``side-effect''.

Rule-based ADR extraction methods were used by Sohn et al. \cite{Sohn}. The clinical Text Analysis and Knowledge Extraction System (cTAKES), an open source NLP system for the medical domain, is used to tag named entities. Named entities that are symptoms or diseases are tagged as potential side effects (PSE) and are used in many of the rules described by Sohn in his paper.

Although not related to ADR detection, Sondhi et al. \cite{Sondhi}  used similar features to our classifier when constructing a classification system to discriminate between two aspects of medical case descriptions, Physical Examination/Symptoms (PE) and Medications (MED), in medical forum data. PEs are defined as conditions for which treatments are being proposed and MEDs are the medications a person is currrently taking or intends to take. Some features used for the classification task include n-gram features, morphological features, and Unified Medical Language System (UMLS) features.

\section{Methods}
The problem we are trying to solve is a supervised learning problem. Thus, we have labeled data and we want to carefully choose features that allow us to discriminate between samples of different labels. The corpus we use was provided by Ask a Patient and it consists of 15,198 sentences where 9,000 are positive samples and 6,198 are negative samples. On the Ask a Patient website, a user's drug review is separated into various fields: rating, reason, side effects for <drug name>, comments, sex, age, duration/dosage, and date added. We consider the side-effects field as positive samples and the comments field as negative samples. Below is an example of an Ask a Patient user submitted review.

\section{Conclusions}
This paragraph will end the body of this sample document.
Remember that you might still have Acknowledgments or
Appendices; brief samples of these
follow.  There is still the Bibliography to deal with; and
we will make a disclaimer about that here: with the exception
of the reference to the \LaTeX\ book, the citations in
this paper are to articles which have nothing to
do with the present subject and are used as
examples only.
%\end{document}  % This is where a 'short' article might terminate

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
This section is optional; it is a location for you
to acknowledge grants, funding, editing assistance and
what have you.  In the present case, for example, the
authors would like to thank Gerald Murray of ACM for
his help in codifying this \textit{Author's Guide}
and the \textbf{.cls} and \textbf{.tex} files that it describes.

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns

% That's all folks!
\end{document}
